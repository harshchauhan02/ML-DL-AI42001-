{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/OFoIJUffwyRlM1vWbEEE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Imported the necessary libraries.\n"],"metadata":{"id":"-VUUjU7-GasE"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from sklearn.model_selection import train_test_split\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.naive_bayes import MultinomialNB"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G4fmSjmc-age","executionInfo":{"status":"ok","timestamp":1677304407673,"user_tz":-330,"elapsed":414,"user":{"displayName":"Harsh Singh Chauhan","userId":"17252824170291030252"}},"outputId":"ff25be5a-7123-4e9e-a9cf-980f6859b446"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["Loaded the datasets.\n"],"metadata":{"id":"HT2lZN3yGf_H"}},{"cell_type":"code","source":["amazon_df = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', header=None)\n","imdb_df = pd.read_csv('imdb_labelled.txt', sep='\\t', header=None)\n","yelp_df = pd.read_csv('yelp_labelled.txt', sep='\\t', header=None)\n"],"metadata":{"id":"gAZC1KiV-ac_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Merged the datasets into a single dataframe.\n"],"metadata":{"id":"NUNZOoEKGi9Q"}},{"cell_type":"code","source":["df = pd.concat([amazon_df, imdb_df, yelp_df])\n","df.columns = ['sentence', 'tag']\n","\n","\n","# Print thee first 5 rows of data\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-CtHVSiX-aay","executionInfo":{"status":"ok","timestamp":1677304274617,"user_tz":-330,"elapsed":2,"user":{"displayName":"Harsh Singh Chauhan","userId":"17252824170291030252"}},"outputId":"b6e5670f-9c99-4ba6-9fa8-81811e5d3aaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                            sentence  tag\n","0  So there is no way for me to plug it in here i...    0\n","1                        Good case, Excellent value.    1\n","2                             Great for the jawbone.    1\n","3  Tied to charger for conversations lasting more...    0\n","4                                  The mic is great.    1\n"]}]},{"cell_type":"markdown","source":["Performed preprocessing on the text data including stopword removal, lower casing, stemming, lemmatization, and tokenization."],"metadata":{"id":"Q4oAyFyDGork"}},{"cell_type":"code","source":["# Tokenization\n","df['sentence'] = df['sentence'].apply(lambda x: re.findall(r'\\b\\w+\\b', x))\n","\n","# Lowercase\n","df['sentence'] = df['sentence'].apply(lambda x: [word.lower() for word in x])\n","\n","# Stopword removal\n","stop_words = stopwords.words('english')\n","df['sentence'] = df['sentence'].apply(lambda x: [word for word in x if word not in stop_words])\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","df['sentence'] = df['sentence'].apply(lambda x: [stemmer.stem(word) for word in x])\n","\n","# Lemmatization\n","lemmatizer = WordNetLemmatizer()\n","df['sentence'] = df['sentence'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n","\n","# Print the first 5 rows of preprocessed data\n","print(df.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGibKP5s-aYE","executionInfo":{"status":"ok","timestamp":1677304279283,"user_tz":-330,"elapsed":1087,"user":{"displayName":"Harsh Singh Chauhan","userId":"17252824170291030252"}},"outputId":"f0e61a60-e644-4f92-bd22-7f8c0b9b93ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                            sentence  tag\n","0                [way, plug, u, unless, go, convert]    0\n","1                          [good, case, excel, valu]    1\n","2                                    [great, jawbon]    1\n","3  [tie, charger, convers, last, 45, minut, major...    0\n","4                                       [mic, great]    1\n"]}]},{"cell_type":"markdown","source":["Created a bag of words representation of the text data.\n"],"metadata":{"id":"0RxKFqULGyvW"}},{"cell_type":"code","source":["# Creating a vocabulary of unique words\n","vocab = set()\n","for sentence in df['sentence']:\n","    for word in sentence:\n","        vocab.add(word)\n","\n","# Create a dictionary to hold the word counts\n","word_count = {word: [0] * len(df['sentence']) for word in vocab}\n","\n","# Loop over each sentence and count the occurrences of each word\n","for i, sentence in enumerate(df['sentence']):\n","    for word in sentence:\n","        word_count[word][i] += 1\n","\n","# Creating a dataframe of the word counts\n","word_count_df = pd.DataFrame(word_count)\n","\n","\n","# Print dataframe of the word count dimensions\n","print(\" dataframe of the word count:\", word_count_df.shape)\n","\n","# Calculate the feature value of each word\n","N = len(df['sentence'])\n","word_count_df = word_count_df.apply(lambda x: np.log(1 + x) * np.log(N / (x > 0).sum()))\n","\n","# Printing number of samples in the data set\n","print(\"Data Set Size:\", len(df))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XEFAEC7k-aVY","executionInfo":{"status":"ok","timestamp":1677304287751,"user_tz":-330,"elapsed":6787,"user":{"displayName":"Harsh Singh Chauhan","userId":"17252824170291030252"}},"outputId":"1981262d-780d-4dd5-c3a8-0e8f13c55b52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" dataframe of the word count: (2748, 3957)\n","Data Set Size: 2748\n"]}]},{"cell_type":"markdown","source":["Split the data into training and test sets with a ratio of 80:20.\n"],"metadata":{"id":"QObfifcfFmyh"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(word_count_df, df['tag'], test_size=0.2, random_state=42)\n"],"metadata":{"id":"FV0DPYNK-aS2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define a Naive Bayes classifier and train it using the training data:"],"metadata":{"id":"wIZbjKlZFhuq"}},{"cell_type":"markdown","source":["Defined a Naive Bayes classifier and trained it on the training data.\n"],"metadata":{"id":"iXdY2P1rHA5D"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WiJkG9hRdqQI","executionInfo":{"status":"ok","timestamp":1677304287752,"user_tz":-330,"elapsed":4,"user":{"displayName":"Harsh Singh Chauhan","userId":"17252824170291030252"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6bea1b27-7b78-40ba-d8a2-f998d7111307"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB()"]},"metadata":{},"execution_count":82}],"source":["\n","\n","# Defining the classifier\n","nb_classifier = MultinomialNB()\n","\n","# Training the classifier using the training data\n","nb_classifier.fit(X_train, y_train)\n"]},{"cell_type":"markdown","source":["Used the trained classifier to make predictions on the test data.\n"],"metadata":{"id":"FTAhZblTFeuC"}},{"cell_type":"code","source":["# Using the trained classifier to make predictions on the test data\n","y_pred = nb_classifier.predict(X_test)\n"],"metadata":{"id":"DURcljoO-sbu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluated the performance of the classifier using accuracy score and classification report."],"metadata":{"id":"ei8WeieIFcXY"}},{"cell_type":"code","source":["\n","\n","# Calculating the accuracy score\n","acc_score = accuracy_score(y_test, y_pred)\n","print(\"Accuracy Score:\", acc_score)\n","\n","print('Confusion Matrix:')\n","print(confusion_matrix(y_test, y_pred))\n","\n","# Generating a classification report\n","class_report = classification_report(y_test, y_pred)\n","print(\"Classification Report:\\n\", class_report)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uFZklM6C-uct","executionInfo":{"status":"ok","timestamp":1677304289603,"user_tz":-330,"elapsed":3,"user":{"displayName":"Harsh Singh Chauhan","userId":"17252824170291030252"}},"outputId":"0906f282-14f7-41e7-f933-0971f3144aa1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Score: 0.7618181818181818\n","Confusion Matrix:\n","[[216  75]\n"," [ 56 203]]\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.79      0.74      0.77       291\n","           1       0.73      0.78      0.76       259\n","\n","    accuracy                           0.76       550\n","   macro avg       0.76      0.76      0.76       550\n","weighted avg       0.76      0.76      0.76       550\n","\n"]}]}]}
